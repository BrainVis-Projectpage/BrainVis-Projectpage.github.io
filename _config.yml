# Site settings
title: BrainVis
description: BrainVis official project site
# baseurl: "" # the subpath of your site, e.g. /blog
# url: "localhost:3000" # the base hostname & protocol for your site e.g. http://willianjusten.com.br

# User settings
username: BrainVis
user_authors: <p><a href="https://romgai.github.io/">Honghao Fu</a><sup>1,2</sup>, <a href="https://personal.ntu.edu.sg/zqshen/">Zhiqi Shen</a><sup>2</sup>, <a href="https://blogs.ntu.edu.sg/nisth/2021/08/23/jing-jih-chin/">Jing Jih Chin</a><sup>2</sup>, <a href="https://wanghao.tech/">Hao Wang</a><sup>1†</sup><br>&nbsp<br><sup>1</sup>The Hong Kong University of Science and Technology (Guangzhou)<br><sup>2</sup>Nanyang Technological University<br>&nbsp<br><sup>†</sup>Corresponding Author</p>
user_abstract: <p class="left-align">Analyzing and reconstructing visual stimuli from brain signals effectively advances understanding of the human visual system. However, the EEG signals are complex and contain a amount of noise. This leads to substantial limitations in existing works of visual stimuli reconstruction from EEG, such as difficulties in aligning EEG embeddings with the fine-grained semantic information and a heavy reliance on additional large self-collected dataset for training. To address these challenges, we propose a novel approach called BrainVis. Firstly, we divide the EEG signals into various units and apply a self-supervised approach on them to obtain EEG time-domain features, in an attempt to ease the training difficulty. Additionally, we also propose to utilize the frequency-domain features to enhance the EEG representations. Then, we simultaneously align EEG time-frequency embeddings with the interpolation of the coarse and fine-grained semantics in the CLIP space, to highlight the primary visual components and reduce the cross-modal alignment difficulty. Finally, we adopt the cascaded diffusion models to reconstruct images. Our proposed BrainVis outperforms state of the arts in both semantic fidelity reconstruction and generation quality. Notably, we reduce the training data scale to 10% of the previous work.</p>
user_framework: <img src="pic2.jpg" alt="Framework"><p class="left-align">We first aim to obtain the time and frequency features for the given EEG signals, in which the time encoder leverages self-supervised pre-training approach with masking segmentation and latent reconstruction, while the frequency encoder employs LSTM to extract features with Fast Fourier Transform (FFT). Then the time and frequency encoders are fine-tuned simultaneously with EEG classifier to obtain time-frequency dual embedding. Following this, an alignment network aligns the EEG time-frequency dual embedding with the fine-grained semantic of visual stimuli image in CLIP space, which is semantic interpolation of the label of visual stimuli image and its caption. Finally, the aligned EEG fine-grained embedding, along with the coarse-grained embedding of classification result, serve as conditions of cascaded diffusion models for multi-level semantic visual reconstruction.</p>
user_Contribution: <p class="left-align"><ul class="left-align"><li>We improve the self-supervised embedding method for EEG time-domain features, eliminating the reliance on additional self-collected large-scale dataset. </li><li>We propose the first EEG visual stimuli reconstruction method that integrates time and frequency-domain features, enhancing the EEG representations.</li><li>BrainVis generates images with higher accuracy and better quality, and also overcomes previous limitation that was limited to only coarse-grained reconstruction.</li></ul></p>
user_Experiment_Results: <p class="left-align">Below are visual stimuli reconstruction results (partial) and the ground truth (GT) images. We are able to not only reconstruct the correct types but also capture their fine-grained semantics to some extent, like prominent numerical, color, environmental, or behavioral features. For more results, please download the supplementary materials</p><img src="pic3.jpg" alt="exp1"><p class="left-align">Below is the comparison of reconstructed images’ quality between our method and prior works.</p><img src="pic4.jpg" alt="exp2"><p class="left-align">Below is the comparison of reconstructed images with the previews work on the same GT image.</p><img src="pic5.jpg" alt="exp3"><p class="left-align">For quantitative experiment, please download the paper.</p>
user_BibTeX: <p class="left-align">```aaaaaaa```</p>
user_Acknowledgments: <p class="left-align">BrainVis builds upon several previous works.<ul class="left-align"><li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">High-resolution image synthesis with latent diffusion models (CVPR 2022)</a></li><li><a href="https://proceedings.mlr.press/v139/radford21a/radford21a.pdf">Learning Transferable Visual Models From Natural Language Supervision (ICML 2021)</a></li><li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Seeing_Beyond_the_Brain_Conditional_Diffusion_Model_With_Sparse_Masked_CVPR_2023_paper.pdf">Masked modeling conditioned diffusion model for human vision decoding (CVPR 2023)</a></li><li><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Spampinato_Deep_Learning_Human_CVPR_2017_paper.pdf">Deep learning human mind for automated visual classification (CVPR 2017)</a></li><li><a href="https://arxiv.org/pdf/2303.00320.pdf">Self-Supervised Representations of Time Series with Decoupled Masked Autoencoders</a></li></ul></p>


user_title: Exploring the bridge between brain and visual signals via image reconstruction
email: hfu006@ntu.edu.sg
# twitter_username: lorem_ipsum
#instagram_username: mauriciourraco7
#facebook_username: mauricio.urraco
#linkedin_username: murraco
github_username:  RomGai/BrainVis
# medium_username: lorem_ipsum
# medium_url: lorem_ipsum.medium.com
# gplus_username:  lorem_ipsum

exclude: ['package.json', 'src', 'node_modules', 'vendor']
